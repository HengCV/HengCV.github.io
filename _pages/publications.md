---
layout: archive
permalink: /publications/
author_profile: true
---
You can find the full list of my publications on <a href="http://scholar.google.com/citations?user=ghmgyewAAAAJ&hl=en"><img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/google_scholar.png" width="100"/></a> and <a href="https://www.semanticscholar.org/author/Heng-Wang/46506697"><img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/semantic_scholar.png" width="100"/></a>. The following are selected publications:

<table style="border: none; border-collapse: collapse;" border="0">
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/uvo.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation.</b>
<br>
Weiyao Wang, Matt Feiszli, <b>Heng Wang</b>, Du Tran. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.04691">Paper</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/dataset?authuser=0">Dataset</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/workshop-program">Workshop</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/challenge-intro">Challenge</a></span>, 
<span><a href="https://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation">Facebook AI Blog</a></span>
</td>
</tr>
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/Auto-TSNet.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Searching for Two-Stream Models in Multivariate Space for Video Recognition.</b>
<br>
Xinyu Gong, <b>Heng Wang</b>, Zheng Shou, Matt Feiszli, Zhangyang Wang, Zhicheng Yan. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2108.12957">Paper</a></span>  
</td>
</tr>

  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/IPL.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Interactive Prototype Learning for Egocentric Action Recognition.</b>
<br>
Xiaohan Wang, Linchao Zhu, <b>Heng Wang</b>, Yi Yang. <b>ICCV</b>, 2021.
<br>
<span><a href="https://ffmpbgrnn.github.io/publications/pdf/ipl.pdf">Paper</a></span> 
</td>
</tr>  
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/timesformer.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Is Space-Time Attention All You Need for Video Understanding?</b>
<br>
Gedas Bertasius, <b>Heng Wang</b>, Lorenzo Torresani. <b>ICML</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2102.05095">Paper</a></span>,  
<span><a href="https://github.com/facebookresearch/TimeSformer">Code</a></span>, 
<span><a href="https://ai.facebook.com/blog/timesformer-a-new-architecture-for-video-understanding/">Facebook AI Blog</a></span>
</td>
</tr>    
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/CM.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories.</b>
<br>
Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry Davis, <b>Heng Wang</b>. <b>CVPR</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.01198">Paper</a></span>,  
<span><a href="poster.pdf">Poster</a></span>, 
<span><a href="presentation_slides.pdf">Slides</a></span>
</td>
</tr>
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/inpainting.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Proposal-based Video Completion.</b>
<br>
Yuan-Ting Hu, <b>Heng Wang</b>, Nicolas Ballas, Kristen Grauman and Alexander G. Schwing. ECCV, 2020.
<br>
<span><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720035.pdf">Paper</a></span>
</td>
</tr>      
 
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/CorrNet.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Video Modeling with Correlation Networks.</b>
<br>
<b>Heng Wang</b>, Du Tran, Lorenzo Torresani and Matt Feiszli. CVPR, 2020.
<br>
<span><a href="https://arxiv.org/abs/1906.03349">Paper</a></span>
</td>
</tr>    
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/FASTER.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>FASTER Recurrent Networks for Efficient Video Classification.</b>
<br>
Linchao Zhu, Laura Sevilla-Lara, Du Tran, Matt Feiszli, Yi Yang and <b>Heng Wang</b>. AAAI, 2020.
<br>
<span><a href="https://arxiv.org/abs/1906.03349">Paper</a></span>
</td>
</tr>   
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/CSN.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Video Classification with Channel-Separated Convolutional Networks.</b>
<br>
Du Tran, <b>Heng Wang</b>, Lorenzo Torresani and Matt Feiszli. ICCV, 2019.
<br>
<span><a href="https://arxiv.org/abs/1904.02811">Paper</a></span>,
<span><a href="https://github.com/facebookresearch/VMZ">Code</a></span>
</td>
</tr>  
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/URU-Video.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Large-scale Weakly-Supervised Pre-training for Video Action Recognition.</b>
<br>
Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, <b>Heng Wang</b>, and Dhruv Mahajan. CVPR, 2019.
<br>
<span><a href="https://arxiv.org/abs/1905.00561">Paper</a></span>
</td>
</tr>   
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/SOA.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Scenes-Objects-Actions: A Multi-Task, Multi-Label Video Dataset.</b>
<br>
Jamie Ray, <b>Heng Wang</b>, Du Tran, Yufei Wang, Matt Feiszli, Lorenzo Torresani, and Manohar Palurin. ECCV, 2018.
<br>
<span><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Heng_Wang_Scenes-Objects-Actions_A_Multi-Task_ECCV_2018_paper.html">Paper</a></span>
</td>
</tr> 
  

<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/R(2+1)D.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>A Closer Look at Spatiotemporal Convolutions for Action Recognition.</b>
<br>
Du Tran, <b>Heng Wang</b>, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri. CVPR, 2018.
<br>
<span><a href="https://arxiv.org/abs/1711.11248">Paper</a></span>,
<span><a href="https://github.com/facebookresearch/VMZ">Code</a></span>,
<span><a href="https://dutran.github.io/R2Plus1D/">Project page</a></span>
</td>
</tr>  


<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/IDT-IJCV.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>A Robust and Efficient Video Representation for Action Recognition.</b>
<br>
<b>Heng Wang</b>, Dan Oneata, Jakob Verbeek, Cordelia Schmid. IJCV, 2016.
<br>
<span><a href="https://hal.inria.fr/hal-01145834/document">Paper</a></span>
</td>
</tr> 
  

<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/IDT.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Action Recognition with Improved Trajectories.</b>
<br>
<b>Heng Wang</b>, Cordelia Schmid. ICCV, 2013.
<br>
<span><a href="https://hal.inria.fr/hal-00873267v2/document">Paper</a></span>,
<span><a href="http://lear.inrialpes.fr/people/wang/download/improved_trajectory_release.tar.gz">Code</a></span>,
<span><a href="http://lear.inrialpes.fr/people/wang/improved_trajectories">Project page</a></span>
</td>
</tr>  


<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/DT-IJCV.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Dense Trajectories and Motion Boundary Descriptors for Action Recognition.</b>
<br>
<b>Heng Wang</strong>, Alexander Klaser, Cordelia Schmid, Cheng-Lin Liu. IJCV, 2013.
<br>
<span><a href="https://hal.inria.fr/hal-00725627v2/document">Paper</a></span>
</td>
</tr> 
 
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/DT.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Action Recognition by Dense Trajectories.</b>
<br>
<b>Heng Wang</b>, Alexander Klaser, Cordelia Schmid, Cheng-Lin Liu. CVPR, 2011.
<br>
<span><a href="https://hal.inria.fr/inria-00583818/document">Paper</a></span>,
<span><a href="http://lear.inrialpes.fr/people/wang/download/dense_trajectory_release_v1.2.tar.gz">Code</a></span>,
<span><a href="http://lear.inrialpes.fr/people/wang/dense_trajectories">Project page</a></span>
</td>
</tr>    

  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/Dense_Sampling.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Evaluation of Local Spatio-temporal Features for Action Recognition.</b>
<br>
<b>Heng Wang</strong>, Muhammad Muneeb Ullah, Alexander Klaser, Ivan Laptev, Cordelia Schmid. BMVC, 2009.
<br>
<span><a href="https://hal.inria.fr/inria-00439769/document">Paper</a></span>
</td>
</tr>
  
  
</table>
