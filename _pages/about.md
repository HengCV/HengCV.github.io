---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research scientist at [Facebook AI Research](https://ai.facebook.com/people/heng-wang). 
I was an early member of the [Amazon Go](https://www.youtube.com/watch?v=NrmMk1Myrxc>) team that built the computer vision system to replace human crashiers for retail. Before moving to US, I was a postdoc in the [LEAR Team, INRIA](http://lear.inrialpes.fr/) with [Cordelia Schmid](https://thoth.inrialpes.fr/~schmid/).
I received my Ph.D. in Computer Vision from [Chinese Academy of Sciences](http://www.nlpr.ia.ac.cn/en/), and B.S. in Electrical Engineering from [Harbin Institute of Technology](http://en.hit.edu.cn/).

My research interests range from low-level vision to high-level vision with a focus on video understanding. You can find more detailed information in my [CV](Heng_Wang_CV.pdf) and my [old homepage](http://lear.inrialpes.fr/people/wang/). You can contact me via my e-mail: <img width="25%" src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/email.png" >.

### Updates
- Release the [UVO dataset](https://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation) and organize a challenge for [Open-World Segmentation @ ICCV 2021](https://sites.google.com/view/unidentified-video-object/challenge-intro).
- <img width="20%" src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/pytorchvideo.png"> is released! Check out the code at [GitHub](https://github.com/facebookresearch/pytorchvideo) and the [offical webiste](https://pytorchvideo.org/).
- Open sourced the [code & model](https://github.com/facebookresearch/TimeSformer) for [TimeSformer](https://ai.facebook.com/blog/timesformer-a-new-architecture-for-video-understanding/).

### Recent publications
<table style="border: none; border-collapse: collapse;" border="0">
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/PTV.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>PyTorchVideo: A Deep Learning Library for Video Understanding.</b>
<br>
Haoqi Fan, Tullie Murrell, <b>Heng Wang</b>, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, Christoph Feichtenhofer. <b>ACM International Conference on Multimedia</b>, 2021.
<br>
<span><a href="">Paper</a></span>, 
<span><a href="https://pytorchvideo.org/">Project page</a></span>, 
<span><a href="https://github.com/facebookresearch/pytorchvideo">Code</a></span>, 
<span><a href="https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/">Facebook AI Blog</a></span>
</td>
</tr>  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/uvo.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation.</b>
<br>
Weiyao Wang, Matt Feiszli, <b>Heng Wang</b>, Du Tran. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.04691">Paper</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/dataset?authuser=0">Dataset</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/workshop-program">Workshop</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/challenge-intro">Challenge</a></span>, 
<span><a href="https://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation">Facebook AI Blog</a></span>
</td>
</tr>
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/Auto-TSNet.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Searching for Two-Stream Models in Multivariate Space for Video Recognition.</b>
<br>
Xinyu Gong, <b>Heng Wang</b>, Zheng Shou, Matt Feiszli, Zhangyang Wang, Zhicheng Yan. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2108.12957">Paper</a></span>  
</td>
</tr>

  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/IPL.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Interactive Prototype Learning for Egocentric Action Recognition.</b>
<br>
Xiaohan Wang, Linchao Zhu, <b>Heng Wang</b>, Yi Yang. <b>ICCV</b>, 2021.
<br>
<span><a href="https://ffmpbgrnn.github.io/publications/pdf/ipl.pdf">Paper</a></span> 
</td>
</tr>  
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/timesformer.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Is Space-Time Attention All You Need for Video Understanding?</b>
<br>
Gedas Bertasius, <b>Heng Wang</b>, Lorenzo Torresani. <b>ICML</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2102.05095">Paper</a></span>,  
<span><a href="https://github.com/facebookresearch/TimeSformer">Code</a></span>, 
<span><a href="https://ai.facebook.com/blog/timesformer-a-new-architecture-for-video-understanding/">Facebook AI Blog</a></span>
</td>
</tr>    
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/CM.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories.</b>
<br>
Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry Davis, <b>Heng Wang</b>. <b>CVPR</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.01198">Paper</a></span>,  
<span><a href="poster.pdf">Poster</a></span>, 
<span><a href="presentation_slides.pdf">Slides</a></span>
</td>
</tr>    

  
</table>

### Professional service
- Area Chair: BMVC 2021
- Reviewer: CVPR'13-21, ICCV'13-21, ECCV'14-20, T-PAMI, IJCV, etc.
