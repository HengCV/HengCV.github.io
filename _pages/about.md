---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a research lead at TikTok. I was a research scientist at [Facebook AI Research](https://ai.facebook.com/people/heng-wang), and an early member of the [Amazon Go](https://www.youtube.com/watch?v=NrmMk1Myrxc>) team that built the computer vision system to replace human cashiers for retail. Before moving to US, I was a postdoc in the [LEAR Team, INRIA](https://thoth.inrialpes.fr/people/wang/) with [Cordelia Schmid](https://thoth.inrialpes.fr/~schmid/).
I received my Ph.D. in Computer Vision from [Chinese Academy of Sciences](http://www.nlpr.ia.ac.cn/en/), and B.S. in Electrical Engineering from [Harbin Institute of Technology](http://en.hit.edu.cn/).

My research interests range from low-level vision to high-level vision with a focus on video understanding. You can find more detailed information in my [CV](Heng_Wang_CV.pdf) and my [old homepage](http://lear.inrialpes.fr/people/wang/). The best way to contact me is via my e-mail: <img width="25%" src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/email.png" >.


### Updates
- Honored to receive the [<b>Helmholtz Prize</b>](https://www.thecvf.com/?page_id=413#Helmholtz) (Test of Time Award) at [ICCV 2023](https://iccv2023.thecvf.com/).
- Congrats to the team for winning the 1st place on the medium track of the [DataComp challenge](https://www.datacomp.ai/leaderboard.html) at [ICCV 2023](https://iccv2023.thecvf.com/). Check out our [arXiv paper](https://arxiv.org/abs/2309.15954) for more details.
- 2 papers at CVPR 2023 and 1 paper at ICCV 2023.
- Release the [code & model](https://github.com/facebookresearch/Generic-Grouping) for our [CVPR 2022 paper](https://sites.google.com/view/generic-grouping) on open-world instance segmentation.
- Release the [UVO dataset](https://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation) and organize a challenge for [Open-World Segmentation @ ICCV 2021](https://sites.google.com/view/unidentified-video-object/challenge-intro).
- <img width="20%" src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/pytorchvideo.png"> is released! Check out the code at [GitHub](https://github.com/facebookresearch/pytorchvideo) and the [offical webiste](https://pytorchvideo.org/).
- Open sourced the [code & model](https://github.com/facebookresearch/TimeSformer) for [TimeSformer](https://ai.facebook.com/blog/timesformer-a-new-architecture-for-video-understanding/).

### Recent publications
<table style="border: none; border-collapse: collapse;" border="0">

<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/PTNL.png" width="250"/> </td>
  
 
<td style="border-collapse: collapse; border: none;">
<b>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</b>
<br>
Cheng-En Wu, Yu Tian, Haichao Yu, <b>Heng Wang</b>, Pedro Morgado, Yu Hen Hu, Linjie Yang. <b>ICCV</b>, 2023.
<br>
<span><a href="https://arxiv.org/abs/2307.11978">Paper</a></span>, 
<span><a href="https://github.com/CEWu/PTNL">Project page</a></span>, 
<span><a href="https://github.com/CEWu/PTNL">Code</a></span>
</td>
</tr> 
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/R2former.png" width="250"/> </td>
  
 
<td style="border-collapse: collapse; border: none;">
<b>R2Former: Unified Retrieval and Reranking Transformer for Place Recognition.</b>
<br>
Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiaohui Shen, <b>Heng Wang</b>. <b>CVPR</b>, 2023.
<br>
<span><a href="https://arxiv.org/abs/2304.03410">Paper</a></span>, 
<span><a href="https://github.com/Jeff-Zilence/R2Former">Project page</a></span>, 
<span><a href="https://github.com/Jeff-Zilence/R2Former">Code</a></span>
</td>
</tr> 
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/panic3d.png" width="250"/> </td>
  
 
<td style="border-collapse: collapse; border: none;">
<b>PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters.</b>
<br>
Shuhong Chen, Kevin Zhang, Yichun Shi, <b>Heng Wang</b>, Yiheng Zhu, Guoxian Song, Sizhe An, Janus Kristjansson, Xiao Yang, Matthias Zwicker. <b>CVPR</b>, 2023.
<br>
<span><a href="https://arxiv.org/abs/2303.14587">Paper</a></span>, 
<span><a href="https://github.com/ShuhongChen/panic3d-anime-reconstruction">Project page</a></span>, 
<span><a href="https://github.com/ShuhongChen/panic3d-anime-reconstruction">Dataset</a></span>
</td>
</tr> 
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/ggn.png" width="250"/> </td>
  
 
<td style="border-collapse: collapse; border: none;">
<b>Open-World Instance Segmentation: Exploiting Pseudo Ground Truth Learned from Pairwise Affinity.</b>
<br>
Weiyao Wang, Matt Feiszli, <b>Heng Wang</b>, Jitendra Malik, Du Tran. <b>CVPR</b>, 2022.
<br>
<span><a href="https://arxiv.org/abs/2204.06107">Paper</a></span>, 
<span><a href="https://sites.google.com/view/generic-grouping">Project page</a></span>, 
<span><a href="https://github.com/facebookresearch/Generic-Grouping">Code</a></span>
</td>
</tr>  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/PTV.png" width="250"/> </td>
  

<td style="border-collapse: collapse; border: none;">
<b>PyTorchVideo: A Deep Learning Library for Video Understanding.</b>
<br>
Haoqi Fan, Tullie Murrell, <b>Heng Wang</b>, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, Christoph Feichtenhofer. <b>ACM International Conference on Multimedia</b>, 2021.
<br>
<span><a href="http://HengCV.github.io/files/pytorchvideo.pdf">Paper</a></span>, 
<span><a href="https://pytorchvideo.org/">Project page</a></span>, 
<span><a href="https://github.com/facebookresearch/pytorchvideo">Code</a></span>, 
<span><a href="https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/">Facebook AI Blog</a></span>
</td>
</tr>  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/uvo.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation.</b>
<br>
Weiyao Wang, Matt Feiszli, <b>Heng Wang</b>, Du Tran. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.04691">Paper</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/dataset?authuser=0">Dataset</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/workshop-program">Workshop</a></span>, 
<span><a href="https://sites.google.com/view/unidentified-video-object/challenge-intro">Challenge</a></span>, 
<span><a href="https://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation">Facebook AI Blog</a></span>
</td>
</tr>
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/Auto-TSNet.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Searching for Two-Stream Models in Multivariate Space for Video Recognition.</b>
<br>
Xinyu Gong, <b>Heng Wang</b>, Zheng Shou, Matt Feiszli, Zhangyang Wang, Zhicheng Yan. <b>ICCV</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2108.12957">Paper</a></span>  
</td>
</tr>

  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/IPL.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Interactive Prototype Learning for Egocentric Action Recognition.</b>
<br>
Xiaohan Wang, Linchao Zhu, <b>Heng Wang</b>, Yi Yang. <b>ICCV</b>, 2021.
<br>
<span><a href="https://ffmpbgrnn.github.io/publications/pdf/ipl.pdf">Paper</a></span> 
</td>
</tr>  
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/timesformer.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Is Space-Time Attention All You Need for Video Understanding?</b>
<br>
Gedas Bertasius, <b>Heng Wang</b>, Lorenzo Torresani. <b>ICML</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2102.05095">Paper</a></span>,  
<span><a href="https://github.com/facebookresearch/TimeSformer">Code</a></span>, 
<span><a href="https://ai.facebook.com/blog/timesformer-a-new-architecture-for-video-understanding/">Facebook AI Blog</a></span>
</td>
</tr>    
  
  
<tr style="border-collapse: separate; border-spacing:30em;">
<td style="border-collapse: collapse; border: none;">
<img src="https://raw.githubusercontent.com/hengcv/hengcv.github.io/master/images/CM.png" width="250"/> </td>

<td style="border-collapse: collapse; border: none;">
<b>Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories.</b>
<br>
Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry Davis, <b>Heng Wang</b>. <b>CVPR</b>, 2021.
<br>
<span><a href="https://arxiv.org/abs/2104.01198">Paper</a></span>,  
<span><a href="poster.pdf">Poster</a></span>, 
<span><a href="presentation_slides.pdf">Slides</a></span>
</td>
</tr>    

  
</table>

### Professional service
- Area Chair: BMVC 2021, ICCV 2023
- Reviewer: CVPR'13-23, ICCV'13-21, ECCV'14-22, NeurIPS, ICML, ICLR, T-PAMI, IJCV, etc.
